{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVPAihG4U1j"
   },
   "source": [
    "# Sesión 8 - Policy Gradients: Implementación de PG, bPG, y AC\n",
    "\n",
    "> En la presente sesión, se va a desarrollar la **implementación**  algoritmos dentro de la familia de **Policy Gradients (PG)** En concreto, se va a implementar **Vanilla Policy Gradients (PG)**, **baseline - Policy Gradients (bPG)**, y **Actor-Critic (AC)**. La implementación será realizada utilizando la librería **pytorch**, librería de más bajo nivel, y la cuál permitirá trabajar en detalle la estrategia de aprendizaje on-policy y la optimización por gradiente de la policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWWcufoC7S2B"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svUw2WiJAUy"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda update --all\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouO30DIAKL3"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw5W3OopAFKN"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/VIU/08_AR_MIAR/sesiones_practicas/sesion_practica_2\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK5sY_ybAFt8"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lN7KLe05NSa"
   },
   "outputs": [],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2zVSAPW43MH"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmVd8L9XBt26"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihTI9TOD43ML"
   },
   "source": [
    "---\n",
    "### 1.5.Acerca de las librerías para RL\n",
    "\n",
    "Librería para trabajar con nuestros entornos: gym (https://gym.openai.com/) \\\n",
    "Librería para trabajar con deep learning: tensorflow (https://www.tensorflow.org/) \\\n",
    "Librería para desarrollar soluciones de RL a alto nivel: keras-rl (https://github.com/keras-rl/keras-rl) \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zauhwt0l24tY"
   },
   "source": [
    "---\n",
    "## **PARTE 2** - *Implementación de algoritmos Policy Gradients*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKaWYAQG9-_u"
   },
   "source": [
    "---\n",
    "### 2.1. Vanilla PG (PG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar device CPU/GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \" + str(device))\n",
    "\n",
    "# Variables\n",
    "model_path = \"breackout_pg.pth\"\n",
    "env_name = \"BreakoutDeterministic-v4\"\n",
    "\n",
    "# Hyperparametros - Entrenamiento\n",
    "EPISODES_TRAINING = 200\n",
    "EPISODES_TESTING = 10\n",
    "GAMMA = 0.99\n",
    "T = 512\n",
    "BS = 16\n",
    "\n",
    "# Hyperparametros - Preprocesado observacion-stado\n",
    "HEIGHT, WIDTH = 84, 84\n",
    "N_FRAMES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir arquitectura del knowledge (CNN)\n",
    "class Actor(torch.nn.Module):\n",
    "  def __init__(self, number_actions=4):\n",
    "    super(Actor, self).__init__()\n",
    "\n",
    "    # Definimos las capas de la red neuronal\n",
    "    # 1) Base Model\n",
    "    self.conv1 = torch.nn.Conv2d(4, 32, 8, stride=4)\n",
    "    self.conv2 = torch.nn.Conv2d(32, 64, 4, stride=2)\n",
    "    self.conv3 = torch.nn.Conv2d(64, 64, 3, stride=1)\n",
    "\n",
    "    # 2) Proyección intermedia (Asumimos que hemos hecho flatten)\n",
    "    self.fc1 = torch.nn.Linear(7*7*64, 512) # Para más generalizable, podria usarse GAP\n",
    "\n",
    "    # 3) Salida\n",
    "    self.actor = torch.nn.Linear(512, number_actions) # number of actions\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # 1) Forward base model\n",
    "    x = torch.nn.functional.relu(self.conv1(x))\n",
    "    x = torch.nn.functional.relu(self.conv2(x))\n",
    "    x = torch.nn.functional.relu(self.conv3(x))\n",
    "\n",
    "    # 2) Flatten + Projection\n",
    "    x = x.view(-1, 7*7*64)\n",
    "    x = torch.nn.functional.relu(self.fc1(x))\n",
    "\n",
    "    # 3) Salida hibrida\n",
    "    policy = self.actor(x) # Aplicaremos softmax más adelante\n",
    "\n",
    "    return policy\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funciones para pre-procesado\n",
    "\n",
    "# Pasar de rgb a nivel de gris y re-escalado\n",
    "def rgb2gray_and_resize(screen, height, width):\n",
    "\n",
    "  # RGB a gris\n",
    "  screen_gray = np.array(np.dot(screen[...,:3], [0.299, 0.587, 0.114]), dtype=np.uint8)\n",
    "  # pixel (1,1) -> r=10, g=20, b=30 -> gray = 10 * 0.299 + 20*0.587 + ...\n",
    "  img_from_array = Image.fromarray(screen_gray)\n",
    "\n",
    "  # Re-escalado de imagen\n",
    "  img_from_array = img_from_array.resize((height, width))\n",
    "\n",
    "  return np.array(img_from_array)\n",
    "\n",
    "# Concatenar secuencias (window_lenght)\n",
    "def update_frame_sequence(state, obs, n_frames=4, width=84, height=84):\n",
    "\n",
    "  # Paso a nivel de gris, re-escalado, y estandarización\n",
    "  obs = np.ascontiguousarray(rgb2gray_and_resize(obs, height, width), dtype=np.float32) / 255\n",
    "  obs = torch.FloatTensor(obs)\n",
    "\n",
    "  # Incorporar a buffer\n",
    "  if state is None: # Inicio, no tenemos ventanas previas - repetimos obs inicial\n",
    "    _state = obs.repeat(n_frames, 1).view(n_frames, width, height)\n",
    "  else: # Tenemos ventanas previas, incorporamos la última ventana\n",
    "    _state = state.view(n_frames, height, width)\n",
    "    _state = torch.cat((_state[1:], obs.view(1, width, height)))\n",
    "\n",
    "  return _state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trayectoria 1/200\n",
      "Numero de steps en la trayectoria: 168 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.28632478117942817\n",
      "Trayectoria 2/200\n",
      "Numero de steps en la trayectoria: 138 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 3/200\n",
      "Numero de steps en la trayectoria: 144 -- Recompensa total del episodio: 0.0\n",
      "8/9 - loss: 0.00\n",
      "Trayectoria 4/200\n",
      "Numero de steps en la trayectoria: 211 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.5727335122915415\n",
      "Trayectoria 5/200\n",
      "Numero de steps en la trayectoria: 132 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 6/200\n",
      "Numero de steps en la trayectoria: 172 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.28001164346933366\n",
      "Trayectoria 7/200\n",
      "Numero de steps en la trayectoria: 174 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.25513680800795554\n",
      "Trayectoria 8/200\n",
      "Numero de steps en la trayectoria: 180 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.24941720136187295\n",
      "Trayectoria 9/200\n",
      "Numero de steps en la trayectoria: 141 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 10/200\n",
      "Numero de steps en la trayectoria: 208 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.4336359982307141\n",
      "Trayectoria 11/200\n",
      "Numero de steps en la trayectoria: 167 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.28326887637376785\n",
      "Trayectoria 12/200\n",
      "Numero de steps en la trayectoria: 203 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.59891150146722796\n",
      "Trayectoria 13/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.9896691995007653\n",
      "Trayectoria 14/200\n",
      "Numero de steps en la trayectoria: 48 -- Recompensa total del episodio: 0.0\n",
      "2/3 - loss: 0.00\n",
      "Trayectoria 15/200\n",
      "Numero de steps en la trayectoria: 177 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.26024695282632654\n",
      "Trayectoria 16/200\n",
      "Numero de steps en la trayectoria: 155 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.25032853583494827\n",
      "Trayectoria 17/200\n",
      "Numero de steps en la trayectoria: 138 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 18/200\n",
      "Numero de steps en la trayectoria: 131 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 19/200\n",
      "Numero de steps en la trayectoria: 202 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.4461561615268398\n",
      "Trayectoria 20/200\n",
      "Numero de steps en la trayectoria: 180 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.25547374107620935\n",
      "Trayectoria 21/200\n",
      "Numero de steps en la trayectoria: 222 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.4287393872554486\n",
      "Trayectoria 22/200\n",
      "Numero de steps en la trayectoria: 178 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.27563040703535083\n",
      "Trayectoria 23/200\n",
      "Numero de steps en la trayectoria: 183 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.26360081678087066\n",
      "Trayectoria 24/200\n",
      "Numero de steps en la trayectoria: 134 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 25/200\n",
      "Numero de steps en la trayectoria: 184 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.23510731214826752\n",
      "Trayectoria 26/200\n",
      "Numero de steps en la trayectoria: 152 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.25560005588663953\n",
      "Trayectoria 27/200\n",
      "Numero de steps en la trayectoria: 215 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.53218552584831526\n",
      "Trayectoria 28/200\n",
      "Numero de steps en la trayectoria: 135 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 29/200\n",
      "Numero de steps en la trayectoria: 138 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 30/200\n",
      "Numero de steps en la trayectoria: 158 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.25315554108884597\n",
      "Trayectoria 31/200\n",
      "Numero de steps en la trayectoria: 160 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.24382401034235957\n",
      "Trayectoria 32/200\n",
      "Numero de steps en la trayectoria: 125 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 33/200\n",
      "Numero de steps en la trayectoria: 209 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.41337330524738026\n",
      "Trayectoria 34/200\n",
      "Numero de steps en la trayectoria: 210 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.5763169114406292\n",
      "Trayectoria 35/200\n",
      "Numero de steps en la trayectoria: 212 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.42963086641751835\n",
      "Trayectoria 36/200\n",
      "Numero de steps en la trayectoria: 135 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 37/200\n",
      "Numero de steps en la trayectoria: 201 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.61832010746002236\n",
      "Trayectoria 38/200\n",
      "Numero de steps en la trayectoria: 124 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 39/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 4.0\n",
      "13/14 - loss: 0.8318999877997807\n",
      "Trayectoria 40/200\n",
      "Numero de steps en la trayectoria: 37 -- Recompensa total del episodio: 0.0\n",
      "1/2 - loss: 0.00\n",
      "Trayectoria 41/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.6844930819102697\n",
      "Trayectoria 42/200\n",
      "Numero de steps en la trayectoria: 11 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 43/200\n",
      "Numero de steps en la trayectoria: 156 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.24383263289928436\n",
      "Trayectoria 44/200\n",
      "Numero de steps en la trayectoria: 180 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.2068461091680963\n",
      "Trayectoria 45/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 4.0\n",
      "13/14 - loss: 1.1184548395020621\n",
      "Trayectoria 46/200\n",
      "Numero de steps en la trayectoria: 124 -- Recompensa total del episodio: 1.0\n",
      "6/7 - loss: 0.35827230342796873\n",
      "Trayectoria 47/200\n",
      "Numero de steps en la trayectoria: 132 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 48/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.71540232428482617\n",
      "Trayectoria 49/200\n",
      "Numero de steps en la trayectoria: 19 -- Recompensa total del episodio: 0.0\n",
      "0/1 - loss: 0.00\n",
      "Trayectoria 50/200\n",
      "Numero de steps en la trayectoria: 160 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.24925239086151124\n",
      "Trayectoria 51/200\n",
      "Numero de steps en la trayectoria: 131 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 52/200\n",
      "Numero de steps en la trayectoria: 127 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 53/200\n",
      "Numero de steps en la trayectoria: 142 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 54/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.69552569942814977\n",
      "Trayectoria 55/200\n",
      "Numero de steps en la trayectoria: 47 -- Recompensa total del episodio: 1.0\n",
      "1/2 - loss: 0.6538901329040527\n",
      "Trayectoria 56/200\n",
      "Numero de steps en la trayectoria: 124 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 57/200\n",
      "Numero de steps en la trayectoria: 211 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.49150691353357756\n",
      "Trayectoria 58/200\n",
      "Numero de steps en la trayectoria: 172 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.28432901203632355\n",
      "Trayectoria 59/200\n",
      "Numero de steps en la trayectoria: 136 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 60/200\n",
      "Numero de steps en la trayectoria: 180 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.25512072105299344\n",
      "Trayectoria 61/200\n",
      "Numero de steps en la trayectoria: 136 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 62/200\n",
      "Numero de steps en la trayectoria: 217 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.5609726584874668\n",
      "Trayectoria 63/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 4.0\n",
      "13/14 - loss: 1.4527661246912822\n",
      "Trayectoria 64/200\n",
      "Numero de steps en la trayectoria: 51 -- Recompensa total del episodio: 0.0\n",
      "2/3 - loss: 0.00\n",
      "Trayectoria 65/200\n",
      "Numero de steps en la trayectoria: 154 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.24921117226282755\n",
      "Trayectoria 66/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.7471189541476112\n",
      "Trayectoria 67/200\n",
      "Numero de steps en la trayectoria: 29 -- Recompensa total del episodio: 0.0\n",
      "0/1 - loss: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trayectoria 68/200\n",
      "Numero de steps en la trayectoria: 150 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.24858109487427604\n",
      "Trayectoria 69/200\n",
      "Numero de steps en la trayectoria: 162 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.25833650156855583\n",
      "Trayectoria 70/200\n",
      "Numero de steps en la trayectoria: 132 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 71/200\n",
      "Numero de steps en la trayectoria: 171 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.26960633546113966\n",
      "Trayectoria 72/200\n",
      "Numero de steps en la trayectoria: 174 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.25806112512946134\n",
      "Trayectoria 73/200\n",
      "Numero de steps en la trayectoria: 141 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 74/200\n",
      "Numero de steps en la trayectoria: 155 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.26886878824896283\n",
      "Trayectoria 75/200\n",
      "Numero de steps en la trayectoria: 178 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.23985476385463376\n",
      "Trayectoria 76/200\n",
      "Numero de steps en la trayectoria: 182 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.28703221001408328\n",
      "Trayectoria 77/200\n",
      "Numero de steps en la trayectoria: 206 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.41997369006276136\n",
      "Trayectoria 78/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.7117953896522523\n",
      "Trayectoria 79/200\n",
      "Numero de steps en la trayectoria: 13 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 80/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.6069952100515366\n",
      "Trayectoria 81/200\n",
      "Numero de steps en la trayectoria: 33 -- Recompensa total del episodio: 0.0\n",
      "1/2 - loss: 0.00\n",
      "Trayectoria 82/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.6148873248270589\n",
      "Trayectoria 83/200\n",
      "Numero de steps en la trayectoria: 4 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 84/200\n",
      "Numero de steps en la trayectoria: 130 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 85/200\n",
      "Numero de steps en la trayectoria: 125 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 86/200\n",
      "Numero de steps en la trayectoria: 201 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.6056581834952036\n",
      "Trayectoria 87/200\n",
      "Numero de steps en la trayectoria: 137 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 88/200\n",
      "Numero de steps en la trayectoria: 220 -- Recompensa total del episodio: 3.0\n",
      "12/13 - loss: 0.5586868776724889\n",
      "Trayectoria 89/200\n",
      "Numero de steps en la trayectoria: 134 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 90/200\n",
      "Numero de steps en la trayectoria: 204 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.46681204438209534\n",
      "Trayectoria 91/200\n",
      "Numero de steps en la trayectoria: 171 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.25928469225764276\n",
      "Trayectoria 92/200\n",
      "Numero de steps en la trayectoria: 181 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.24197218363935297\n",
      "Trayectoria 93/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 1.0260329097509384\n",
      "Trayectoria 94/200\n",
      "Numero de steps en la trayectoria: 50 -- Recompensa total del episodio: 0.0\n",
      "2/3 - loss: 0.00\n",
      "Trayectoria 95/200\n",
      "Numero de steps en la trayectoria: 132 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 96/200\n",
      "Numero de steps en la trayectoria: 218 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.5851606978819921\n",
      "Trayectoria 97/200\n",
      "Numero de steps en la trayectoria: 138 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 98/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 1.0569751730987005\n",
      "Trayectoria 99/200\n",
      "Numero de steps en la trayectoria: 54 -- Recompensa total del episodio: 0.0\n",
      "2/3 - loss: 0.00\n",
      "Trayectoria 100/200\n",
      "Numero de steps en la trayectoria: 167 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.25052030533552174\n",
      "Trayectoria 101/200\n",
      "Numero de steps en la trayectoria: 177 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.25504940070889215\n",
      "Trayectoria 102/200\n",
      "Numero de steps en la trayectoria: 133 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 103/200\n",
      "Numero de steps en la trayectoria: 207 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.40450728187958486\n",
      "Trayectoria 104/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.7584100450788226\n",
      "Trayectoria 105/200\n",
      "Numero de steps en la trayectoria: 40 -- Recompensa total del episodio: 0.0\n",
      "1/2 - loss: 0.00\n",
      "Trayectoria 106/200\n",
      "Numero de steps en la trayectoria: 134 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 107/200\n",
      "Numero de steps en la trayectoria: 214 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.5025076086704547\n",
      "Trayectoria 108/200\n",
      "Numero de steps en la trayectoria: 133 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 109/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.56080243842942385\n",
      "Trayectoria 110/200\n",
      "Numero de steps en la trayectoria: 20 -- Recompensa total del episodio: 0.0\n",
      "0/1 - loss: 0.00\n",
      "Trayectoria 111/200\n",
      "Numero de steps en la trayectoria: 172 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.23637441694736483\n",
      "Trayectoria 112/200\n",
      "Numero de steps en la trayectoria: 136 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 113/200\n",
      "Numero de steps en la trayectoria: 140 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 114/200\n",
      "Numero de steps en la trayectoria: 159 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.28284820086426204\n",
      "Trayectoria 115/200\n",
      "Numero de steps en la trayectoria: 129 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 116/200\n",
      "Numero de steps en la trayectoria: 180 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.26136715506965463\n",
      "Trayectoria 117/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.41894509164350374\n",
      "Trayectoria 118/200\n",
      "Numero de steps en la trayectoria: 34 -- Recompensa total del episodio: 1.0\n",
      "1/2 - loss: 0.49276115000247955\n",
      "Trayectoria 119/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.40917058395487926\n",
      "Trayectoria 120/200\n",
      "Numero de steps en la trayectoria: 9 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 121/200\n",
      "Numero de steps en la trayectoria: 144 -- Recompensa total del episodio: 0.0\n",
      "8/9 - loss: 0.00\n",
      "Trayectoria 122/200\n",
      "Numero de steps en la trayectoria: 154 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.26953580644395625\n",
      "Trayectoria 123/200\n",
      "Numero de steps en la trayectoria: 150 -- Recompensa total del episodio: 0.0\n",
      "8/9 - loss: 0.00\n",
      "Trayectoria 124/200\n",
      "Numero de steps en la trayectoria: 133 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 125/200\n",
      "Numero de steps en la trayectoria: 127 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 126/200\n",
      "Numero de steps en la trayectoria: 160 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.23819548860192327\n",
      "Trayectoria 127/200\n",
      "Numero de steps en la trayectoria: 135 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 128/200\n",
      "Numero de steps en la trayectoria: 157 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.22138629274235834\n",
      "Trayectoria 129/200\n",
      "Numero de steps en la trayectoria: 152 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.25084555645783746\n",
      "Trayectoria 130/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.5688818054539816\n",
      "Trayectoria 131/200\n",
      "Numero de steps en la trayectoria: 10 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 132/200\n",
      "Numero de steps en la trayectoria: 124 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 133/200\n",
      "Numero de steps en la trayectoria: 139 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 134/200\n",
      "Numero de steps en la trayectoria: 180 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.26562519913369964\n",
      "Trayectoria 135/200\n",
      "Numero de steps en la trayectoria: 197 -- Recompensa total del episodio: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12 - loss: 0.25764200588067379\n",
      "Trayectoria 136/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.5568393394351006\n",
      "Trayectoria 137/200\n",
      "Numero de steps en la trayectoria: 53 -- Recompensa total del episodio: 1.0\n",
      "2/3 - loss: 0.47395081321398425\n",
      "Trayectoria 138/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.5621513639177594\n",
      "Trayectoria 139/200\n",
      "Numero de steps en la trayectoria: 11 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 140/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.99513370650155254\n",
      "Trayectoria 141/200\n",
      "Numero de steps en la trayectoria: 43 -- Recompensa total del episodio: 0.0\n",
      "1/2 - loss: 0.00\n",
      "Trayectoria 142/200\n",
      "Numero de steps en la trayectoria: 182 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.26290538907051086\n",
      "Trayectoria 143/200\n",
      "Numero de steps en la trayectoria: 158 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.25026881280872562\n",
      "Trayectoria 144/200\n",
      "Numero de steps en la trayectoria: 204 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.6090821300943693\n",
      "Trayectoria 145/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.69081920172486997\n",
      "Trayectoria 146/200\n",
      "Numero de steps en la trayectoria: 5 -- Recompensa total del episodio: 0.0\n",
      "13/0 - loss: 0\n",
      "Trayectoria 147/200\n",
      "Numero de steps en la trayectoria: 167 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.22903207093477249\n",
      "Trayectoria 148/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.55300176143646258\n",
      "Trayectoria 149/200\n",
      "Numero de steps en la trayectoria: 33 -- Recompensa total del episodio: 1.0\n",
      "1/2 - loss: 0.37171648442745217\n",
      "Trayectoria 150/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.5365003666707447\n",
      "Trayectoria 151/200\n",
      "Numero de steps en la trayectoria: 39 -- Recompensa total del episodio: 1.0\n",
      "1/2 - loss: 0.21203602850437164\n",
      "Trayectoria 152/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 2.0\n",
      "13/14 - loss: 0.42928318466459003\n",
      "Trayectoria 153/200\n",
      "Numero de steps en la trayectoria: 30 -- Recompensa total del episodio: 1.0\n",
      "0/1 - loss: 0.32745635509490967\n",
      "Trayectoria 154/200\n",
      "Numero de steps en la trayectoria: 144 -- Recompensa total del episodio: 0.0\n",
      "8/9 - loss: 0.00\n",
      "Trayectoria 155/200\n",
      "Numero de steps en la trayectoria: 193 -- Recompensa total del episodio: 1.0\n",
      "11/12 - loss: 0.26964317013820016\n",
      "Trayectoria 156/200\n",
      "Numero de steps en la trayectoria: 129 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 157/200\n",
      "Numero de steps en la trayectoria: 164 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.27270080745220196\n",
      "Trayectoria 158/200\n",
      "Numero de steps en la trayectoria: 166 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.24419019073247915\n",
      "Trayectoria 159/200\n",
      "Numero de steps en la trayectoria: 157 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.24608576711681154\n",
      "Trayectoria 160/200\n",
      "Numero de steps en la trayectoria: 203 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.4106214108566443\n",
      "Trayectoria 161/200\n",
      "Numero de steps en la trayectoria: 201 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.41481257478396094\n",
      "Trayectoria 162/200\n",
      "Numero de steps en la trayectoria: 132 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 163/200\n",
      "Numero de steps en la trayectoria: 174 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.27378043420612813\n",
      "Trayectoria 164/200\n",
      "Numero de steps en la trayectoria: 215 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.43282823608471896\n",
      "Trayectoria 165/200\n",
      "Numero de steps en la trayectoria: 160 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.23626016452908516\n",
      "Trayectoria 166/200\n",
      "Numero de steps en la trayectoria: 207 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.43736285095413524\n",
      "Trayectoria 167/200\n",
      "Numero de steps en la trayectoria: 136 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 168/200\n",
      "Numero de steps en la trayectoria: 140 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 169/200\n",
      "Numero de steps en la trayectoria: 176 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.2166346467354081\n",
      "Trayectoria 170/200\n",
      "Numero de steps en la trayectoria: 140 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 171/200\n",
      "Numero de steps en la trayectoria: 133 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 172/200\n",
      "Numero de steps en la trayectoria: 194 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.43515506635109585\n",
      "Trayectoria 173/200\n",
      "Numero de steps en la trayectoria: 129 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 174/200\n",
      "Numero de steps en la trayectoria: 128 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 175/200\n",
      "Numero de steps en la trayectoria: 211 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.6018881774865663\n",
      "Trayectoria 176/200\n",
      "Numero de steps en la trayectoria: 197 -- Recompensa total del episodio: 1.0\n",
      "11/12 - loss: 0.23527942163248858\n",
      "Trayectoria 177/200\n",
      "Numero de steps en la trayectoria: 210 -- Recompensa total del episodio: 2.0\n",
      "12/13 - loss: 0.44758077997427725\n",
      "Trayectoria 178/200\n",
      "Numero de steps en la trayectoria: 131 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 179/200\n",
      "Numero de steps en la trayectoria: 172 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.23925473019480703\n",
      "Trayectoria 180/200\n",
      "Numero de steps en la trayectoria: 205 -- Recompensa total del episodio: 2.0\n",
      "11/12 - loss: 0.3949676776925723\n",
      "Trayectoria 181/200\n",
      "Numero de steps en la trayectoria: 189 -- Recompensa total del episodio: 2.0\n",
      "10/11 - loss: 0.5492112758484753\n",
      "Trayectoria 182/200\n",
      "Numero de steps en la trayectoria: 130 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 183/200\n",
      "Numero de steps en la trayectoria: 187 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.26391082189299847\n",
      "Trayectoria 184/200\n",
      "Numero de steps en la trayectoria: 133 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 185/200\n",
      "Numero de steps en la trayectoria: 166 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.29660947620868683\n",
      "Trayectoria 186/200\n",
      "Numero de steps en la trayectoria: 134 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 187/200\n",
      "Numero de steps en la trayectoria: 179 -- Recompensa total del episodio: 1.0\n",
      "10/11 - loss: 0.23932642421939157\n",
      "Trayectoria 188/200\n",
      "Numero de steps en la trayectoria: 157 -- Recompensa total del episodio: 1.0\n",
      "8/9 - loss: 0.24468396852413812\n",
      "Trayectoria 189/200\n",
      "Numero de steps en la trayectoria: 126 -- Recompensa total del episodio: 0.0\n",
      "6/7 - loss: 0.00\n",
      "Trayectoria 190/200\n",
      "Numero de steps en la trayectoria: 130 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 191/200\n",
      "Numero de steps en la trayectoria: 128 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 192/200\n",
      "Numero de steps en la trayectoria: 134 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 193/200\n",
      "Numero de steps en la trayectoria: 128 -- Recompensa total del episodio: 0.0\n",
      "7/8 - loss: 0.00\n",
      "Trayectoria 194/200\n",
      "Numero de steps en la trayectoria: 223 -- Recompensa total del episodio: 3.0\n",
      "12/13 - loss: 0.5209378233322731\n",
      "Trayectoria 195/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.5557462530476707\n",
      "Trayectoria 196/200\n",
      "Numero de steps en la trayectoria: 101 -- Recompensa total del episodio: 6.0\n",
      "5/6 - loss: 1.6782659888267515\n",
      "Trayectoria 197/200\n",
      "Numero de steps en la trayectoria: 157 -- Recompensa total del episodio: 0.0\n",
      "8/9 - loss: 0.00\n",
      "Trayectoria 198/200\n",
      "Numero de steps en la trayectoria: 162 -- Recompensa total del episodio: 1.0\n",
      "9/10 - loss: 0.23816487118601798\n",
      "Trayectoria 199/200\n",
      "Numero de steps en la trayectoria: 224 -- Recompensa total del episodio: 3.0\n",
      "13/14 - loss: 0.8084553565297819\n",
      "Trayectoria 200/200\n",
      "Numero de steps en la trayectoria: 59 -- Recompensa total del episodio: 0.0\n",
      "2/3 - loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento de agente PG:\n",
    "torch.manual_seed(123) # Reproducibilidad\n",
    "\n",
    "# Instanciamos un entorno\n",
    "env = gym.make(env_name)\n",
    "env.seed(123)\n",
    "number_actions = env.action_space.n\n",
    "\n",
    "# Instanciamos un entorno\n",
    "env = gym.make(env_name)\n",
    "number_actions = env.action_space.n\n",
    "\n",
    "# Instanciamos el modelo del actor\n",
    "model = Actor(number_actions=number_actions).to(device)\n",
    "model.eval() # en train, tendriamos que hacer: model.train()\n",
    "\n",
    "# Preparar optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "# Inicializamos primera trayectoria\n",
    "obs, state = env.reset(), None\n",
    "state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "\n",
    "# Primer bucle: episodios de entrenamiento\n",
    "for i_episode in range(int(EPISODES_TRAINING)):\n",
    "  done = False\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # A. Recopilar trayectoria\n",
    "\n",
    "  # Buffer de memoria (s,a,r,s)\n",
    "  s, a, r, lives = [], [], [], [] # Lives is enviroment-specific information\n",
    "\n",
    "  # Segundo bucle: recopilamos la trayectoria\n",
    "  for step in range(int(T)):\n",
    "\n",
    "    # Foward de actor dado el estado actual\n",
    "    with torch.no_grad():\n",
    "        logits = model(state.unsqueeze(0).to(device))\n",
    "\n",
    "    # Obtenemos probabilidad de acción\n",
    "    prob = torch.nn.functional.softmax(logits, -1) # [0, 0.2, 0.6, 0.2]\n",
    "\n",
    "    # Obtener la acción a realizar\n",
    "    action = prob.multinomial(num_samples=1) # 2\n",
    "\n",
    "    # Con la accion seleccionada, realizamos un step en el entorno\n",
    "    obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "    # Almacenamos información en memoria\n",
    "    a_ohe = torch.nn.functional.one_hot(action, num_classes=number_actions).squeeze(0) # Actions to ohe\n",
    "    s.append(state.unsqueeze(0).numpy()), a.append(a_ohe.numpy()), r.append(reward), lives.append(info['ale.lives'])\n",
    "    \n",
    "    # Actualizamos el estado con la siguiente observacion\n",
    "    state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "    \n",
    "    if done:\n",
    "      obs, state = env.reset(), None\n",
    "      state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "      break\n",
    "    \n",
    "  # Printear evolución\n",
    "  print(\"Trayectoria \" + str(int(i_episode)+1) + \"/\" + str(int(EPISODES_TRAINING)))\n",
    "  print(\"Numero de steps en la trayectoria: \" + str(len(r)) + \" -- Recompensa total del episodio: \" + str(np.sum(r)))\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # B. Preparar la información\n",
    "\n",
    "  # Computar los discount rewards\n",
    "  running_r = 0\n",
    "  discount_rewards = r.copy()\n",
    "  for i in reversed(range(len(r))):\n",
    "    if lives[i] != lives[i-1]:\n",
    "        running_r = 0\n",
    "    # Obtenemos discount rewards para cada t\n",
    "    running_r = r[i] + GAMMA * running_r\n",
    "    discount_rewards[i] = running_r\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # C. Entrenar policy con trayectoria recolectada\n",
    "  model.train()\n",
    "  BS = 16\n",
    "\n",
    "  idx, track_loss = 0, 0\n",
    "  indexes = np.arange(0, len(s))\n",
    "  np.random.shuffle(indexes)\n",
    "  for i_epoch in range(len(s)//BS):\n",
    "      indexes_batch = indexes[idx:idx+BS]\n",
    "\n",
    "      # Seleccionamos un batch de la trayectoria\n",
    "      states_batch = np_to_tensor(np.concatenate(s)[indexes_batch,:,:,:])\n",
    "      actions_batch = np_to_tensor(np.concatenate(a)[indexes_batch,:].squeeze())\n",
    "      relevance_batch = np_to_tensor(np.array(discount_rewards)[indexes_batch])\n",
    "    \n",
    "      # Hacemos forward al actor-critic dado el estado actual\n",
    "      logits = model(states_batch)\n",
    "\n",
    "      # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "      log_softmax = torch.nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "      # Obtenemos criterios de optimización\n",
    "      policy_loss = - torch.mean((log_softmax * actions_batch.detach()).sum(-1) * relevance_batch.detach())\n",
    "\n",
    "      # Computamos gradientes\n",
    "      policy_loss.backward()\n",
    "      # Actualizamos los pesos\n",
    "      optimizer.step()\n",
    "      # Limpiamos gradientes del modelo\n",
    "      optimizer.zero_grad()\n",
    "      # Actualizamos iterador de batch\n",
    "      idx += BS\n",
    "      track_loss += policy_loss.item()/(len(s)//BS)\n",
    "      \n",
    "      # Track losses\n",
    "      print(str(i_epoch) + \"/\" + str(len(s)//BS) + \" - loss: \" + str(policy_loss.item()), end=\"\\r\")\n",
    "        \n",
    "  # Overall loss\n",
    "  print(str(i_epoch) + \"/\" + str(len(s)//BS) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    " \n",
    "  # Guardar los pesos del modelo\n",
    "  torch.save(model.state_dict(), model_path)\n",
    "\n",
    "  # Set again model to eval\n",
    "  model.eval()\n",
    "\n",
    "# Guardar los pesos del modelo\n",
    "torch.save(model.state_dict(), \"last\" + model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/10, score: 2.0\n",
      "episode: 1/10, score: 2.0\n",
      "episode: 2/10, score: 0.0\n",
      "episode: 3/10, score: 0.0\n",
      "episode: 4/10, score: 3.0\n",
      "episode: 5/10, score: 1.0\n",
      "episode: 6/10, score: 0.0\n",
      "episode: 7/10, score: 1.0\n",
      "episode: 8/10, score: 0.0\n",
      "episode: 9/10, score: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Testeo\n",
    "def test(env, model, runs):\n",
    "    model.eval()\n",
    "\n",
    "    for e in range(runs):\n",
    "        obs, state = env.reset(), None\n",
    "        state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "        done, score = False, 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            \n",
    "            # Al testear, seleccionamos la acción con mayor probabilidad (no muestreo)\n",
    "            with torch.no_grad():\n",
    "                logits = model(state.unsqueeze(0).to(device))\n",
    "                # Obtenemos probabilidad de acción\n",
    "                prob = torch.nn.functional.softmax(logits, -1)\n",
    "                # Obtener la acción a realizar\n",
    "                action = prob.multinomial(num_samples=1)\n",
    "                \n",
    "            # Con la accion seleccionada, realizamos un step en el entorno\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            # Actualizamos el estado con la siguiente observacion\n",
    "            state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                env.close()\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, runs, score))\n",
    "                break\n",
    "    \n",
    "# Inferencia en test\n",
    "env = gym.make(env_name)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "test(env, model, EPISODES_TESTING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBaq7SGb-EFi"
   },
   "source": [
    "---\n",
    "### 2.2. Baseline Policy Gradients (bPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "model_path = \"breackout_bpg.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trayectoria 1/200\n",
      "Numero de steps en la trayectoria: 168 -- Recompensa total del episodio: 1.0\n",
      "10/10 - loss: 0.002677291631698611\n",
      "Trayectoria 2/200\n",
      "Numero de steps en la trayectoria: 138 -- Recompensa total del episodio: 0.0\n",
      "8/8 - loss: 0.00\n",
      "Trayectoria 3/200\n",
      "Numero de steps en la trayectoria: 144 -- Recompensa total del episodio: 0.0\n",
      "9/9 - loss: 0.00\n",
      "Trayectoria 4/200\n",
      "Numero de steps en la trayectoria: 211 -- Recompensa total del episodio: 2.0\n",
      "13/13 - loss: -0.007684062879819152\n",
      "Trayectoria 5/200\n",
      "Numero de steps en la trayectoria: 132 -- Recompensa total del episodio: 0.0\n",
      "8/8 - loss: 0.00\n",
      "Trayectoria 6/200\n",
      "Numero de steps en la trayectoria: 214 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.0040487675712658905\n",
      "Trayectoria 7/200\n",
      "Numero de steps en la trayectoria: 186 -- Recompensa total del episodio: 1.0\n",
      "11/11 - loss: -0.03549315035343168\n",
      "Trayectoria 8/200\n",
      "Numero de steps en la trayectoria: 226 -- Recompensa total del episodio: 2.0\n",
      "14/14 - loss: -0.0631856849150998228\n",
      "Trayectoria 9/200\n",
      "Numero de steps en la trayectoria: 512 -- Recompensa total del episodio: 3.0\n",
      "32/32 - loss: 0.0272637481684796526\n",
      "Trayectoria 10/200\n",
      "Numero de steps en la trayectoria: 64 -- Recompensa total del episodio: 4.0\n",
      "4/4 - loss: -0.050057468004524716\n",
      "Trayectoria 11/200\n",
      "Numero de steps en la trayectoria: 512 -- Recompensa total del episodio: 3.0\n",
      "32/32 - loss: 0.03193233971251175532\n",
      "Trayectoria 12/200\n",
      "Numero de steps en la trayectoria: 154 -- Recompensa total del episodio: 8.0\n",
      "9/9 - loss: 0.070837773382663735\n",
      "Trayectoria 13/200\n",
      "Numero de steps en la trayectoria: 290 -- Recompensa total del episodio: 7.0\n",
      "18/18 - loss: 0.020472050954898194\n",
      "Trayectoria 14/200\n",
      "Numero de steps en la trayectoria: 200 -- Recompensa total del episodio: 2.0\n",
      "12/12 - loss: 0.008529154583811768\n",
      "Trayectoria 15/200\n",
      "Numero de steps en la trayectoria: 239 -- Recompensa total del episodio: 2.0\n",
      "14/14 - loss: -0.018857227904455993\n",
      "Trayectoria 16/200\n",
      "Numero de steps en la trayectoria: 147 -- Recompensa total del episodio: 0.0\n",
      "9/9 - loss: 0.00\n",
      "Trayectoria 17/200\n",
      "Numero de steps en la trayectoria: 162 -- Recompensa total del episodio: 1.0\n",
      "10/10 - loss: -0.02130477428436279\n",
      "Trayectoria 18/200\n",
      "Numero de steps en la trayectoria: 130 -- Recompensa total del episodio: 0.0\n",
      "8/8 - loss: 0.00\n",
      "Trayectoria 19/200\n",
      "Numero de steps en la trayectoria: 131 -- Recompensa total del episodio: 0.0\n",
      "8/8 - loss: 0.00\n",
      "Trayectoria 20/200\n",
      "Numero de steps en la trayectoria: 204 -- Recompensa total del episodio: 2.0\n",
      "12/12 - loss: -0.02173484178880852\n",
      "Trayectoria 21/200\n",
      "Numero de steps en la trayectoria: 257 -- Recompensa total del episodio: 3.0\n",
      "16/16 - loss: -0.0028699468821287155\n",
      "Trayectoria 22/200\n",
      "Numero de steps en la trayectoria: 221 -- Recompensa total del episodio: 2.0\n",
      "13/13 - loss: -0.065868214345895324\n",
      "Trayectoria 23/200\n",
      "Numero de steps en la trayectoria: 321 -- Recompensa total del episodio: 5.0\n",
      "20/20 - loss: 0.013815958611667164\n",
      "Trayectoria 24/200\n",
      "Numero de steps en la trayectoria: 270 -- Recompensa total del episodio: 4.0\n",
      "16/16 - loss: -0.02687075501307845\n",
      "Trayectoria 25/200\n",
      "Numero de steps en la trayectoria: 196 -- Recompensa total del episodio: 2.0\n",
      "12/12 - loss: 0.008975248783826821\n",
      "Trayectoria 26/200\n",
      "Numero de steps en la trayectoria: 280 -- Recompensa total del episodio: 3.0\n",
      "17/17 - loss: -0.00016577760962879426\n",
      "Trayectoria 27/200\n",
      "Numero de steps en la trayectoria: 333 -- Recompensa total del episodio: 11.0\n",
      "20/20 - loss: 0.021855361294001337\n",
      "Trayectoria 28/200\n",
      "Numero de steps en la trayectoria: 337 -- Recompensa total del episodio: 8.0\n",
      "21/21 - loss: 0.00400866231038457578\n",
      "Trayectoria 29/200\n",
      "Numero de steps en la trayectoria: 387 -- Recompensa total del episodio: 6.0\n",
      "24/24 - loss: 0.01900724996812642994\n",
      "Trayectoria 30/200\n",
      "Numero de steps en la trayectoria: 289 -- Recompensa total del episodio: 4.0\n",
      "18/18 - loss: -0.03438260696000521\n",
      "Trayectoria 31/200\n",
      "Numero de steps en la trayectoria: 263 -- Recompensa total del episodio: 3.0\n",
      "16/16 - loss: -0.0023023055400699377\n",
      "Trayectoria 32/200\n",
      "Numero de steps en la trayectoria: 245 -- Recompensa total del episodio: 3.0\n",
      "15/15 - loss: 0.025724736352761597\n",
      "Trayectoria 33/200\n",
      "Numero de steps en la trayectoria: 291 -- Recompensa total del episodio: 4.0\n",
      "18/18 - loss: -0.023491313474045858\n",
      "Trayectoria 34/200\n",
      "Numero de steps en la trayectoria: 236 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: 0.029319393847669874\n",
      "Trayectoria 35/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 3.0\n",
      "15/15 - loss: 0.028299034883578665\n",
      "Trayectoria 36/200\n",
      "Numero de steps en la trayectoria: 386 -- Recompensa total del episodio: 6.0\n",
      "24/24 - loss: -0.017497683409601457\n",
      "Trayectoria 37/200\n",
      "Numero de steps en la trayectoria: 227 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: -0.040263241955212185\n",
      "Trayectoria 38/200\n",
      "Numero de steps en la trayectoria: 221 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.028948355752688185\n",
      "Trayectoria 39/200\n",
      "Numero de steps en la trayectoria: 331 -- Recompensa total del episodio: 12.0\n",
      "20/20 - loss: -0.013374565355479713\n",
      "Trayectoria 40/200\n",
      "Numero de steps en la trayectoria: 363 -- Recompensa total del episodio: 11.0\n",
      "22/22 - loss: 0.0405772150578824937\n",
      "Trayectoria 41/200\n",
      "Numero de steps en la trayectoria: 288 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: 0.0045661210185951595\n",
      "Trayectoria 42/200\n",
      "Numero de steps en la trayectoria: 267 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.024947471451014286\n",
      "Trayectoria 43/200\n",
      "Numero de steps en la trayectoria: 263 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.02270839735865593\n",
      "Trayectoria 44/200\n",
      "Numero de steps en la trayectoria: 343 -- Recompensa total del episodio: 9.0\n",
      "21/21 - loss: -0.057762197839717075\n",
      "Trayectoria 45/200\n",
      "Numero de steps en la trayectoria: 421 -- Recompensa total del episodio: 11.0\n",
      "26/26 - loss: 0.0485227471933915168\n",
      "Trayectoria 46/200\n",
      "Numero de steps en la trayectoria: 301 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: -0.007059267204668794\n",
      "Trayectoria 47/200\n",
      "Numero de steps en la trayectoria: 274 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: 0.014064205043456135\n",
      "Trayectoria 48/200\n",
      "Numero de steps en la trayectoria: 260 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.001640192640479654\n",
      "Trayectoria 49/200\n",
      "Numero de steps en la trayectoria: 242 -- Recompensa total del episodio: 3.0\n",
      "15/15 - loss: 0.0110421357055505053\n",
      "Trayectoria 50/200\n",
      "Numero de steps en la trayectoria: 225 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: -0.067717717162200396\n",
      "Trayectoria 51/200\n",
      "Numero de steps en la trayectoria: 298 -- Recompensa total del episodio: 7.0\n",
      "18/18 - loss: 0.0070725362747907646\n",
      "Trayectoria 52/200\n",
      "Numero de steps en la trayectoria: 264 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.019567297305911786\n",
      "Trayectoria 53/200\n",
      "Numero de steps en la trayectoria: 332 -- Recompensa total del episodio: 8.0\n",
      "20/20 - loss: 0.029645876772701745\n",
      "Trayectoria 54/200\n",
      "Numero de steps en la trayectoria: 306 -- Recompensa total del episodio: 8.0\n",
      "19/19 - loss: -0.008728813772138792\n",
      "Trayectoria 55/200\n",
      "Numero de steps en la trayectoria: 221 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.08830321465547268\n",
      "Trayectoria 56/200\n",
      "Numero de steps en la trayectoria: 345 -- Recompensa total del episodio: 5.0\n",
      "21/21 - loss: 0.090998112090996346\n",
      "Trayectoria 57/200\n",
      "Numero de steps en la trayectoria: 289 -- Recompensa total del episodio: 7.0\n",
      "18/18 - loss: 0.00032047679026920434\n",
      "Trayectoria 58/200\n",
      "Numero de steps en la trayectoria: 255 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.022158702711264317\n",
      "Trayectoria 59/200\n",
      "Numero de steps en la trayectoria: 317 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: -0.035117953231460173\n",
      "Trayectoria 60/200\n",
      "Numero de steps en la trayectoria: 395 -- Recompensa total del episodio: 6.0\n",
      "24/24 - loss: 0.027755187669148054\n",
      "Trayectoria 61/200\n",
      "Numero de steps en la trayectoria: 257 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.040845921146683395\n",
      "Trayectoria 62/200\n",
      "Numero de steps en la trayectoria: 364 -- Recompensa total del episodio: 6.0\n",
      "22/22 - loss: 0.023857136680321254\n",
      "Trayectoria 63/200\n",
      "Numero de steps en la trayectoria: 186 -- Recompensa total del episodio: 2.0\n",
      "11/11 - loss: 0.003180664710023179\n",
      "Trayectoria 64/200\n",
      "Numero de steps en la trayectoria: 245 -- Recompensa total del episodio: 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - loss: -0.043336061139901495\n",
      "Trayectoria 65/200\n",
      "Numero de steps en la trayectoria: 261 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.013548823771998286\n",
      "Trayectoria 66/200\n",
      "Numero de steps en la trayectoria: 264 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.0043425295734778056\n",
      "Trayectoria 67/200\n",
      "Numero de steps en la trayectoria: 290 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: 0.020772629727919892\n",
      "Trayectoria 68/200\n",
      "Numero de steps en la trayectoria: 288 -- Recompensa total del episodio: 7.0\n",
      "18/18 - loss: -0.022449696643484954\n",
      "Trayectoria 69/200\n",
      "Numero de steps en la trayectoria: 300 -- Recompensa total del episodio: 8.0\n",
      "18/18 - loss: 0.002161750776900187\n",
      "Trayectoria 70/200\n",
      "Numero de steps en la trayectoria: 310 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: -0.01881230504889236\n",
      "Trayectoria 71/200\n",
      "Numero de steps en la trayectoria: 282 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: -0.010475788922870863\n",
      "Trayectoria 72/200\n",
      "Numero de steps en la trayectoria: 252 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.013715886076291465\n",
      "Trayectoria 73/200\n",
      "Numero de steps en la trayectoria: 305 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: -0.020760486588666305\n",
      "Trayectoria 74/200\n",
      "Numero de steps en la trayectoria: 264 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.026488314615562558\n",
      "Trayectoria 75/200\n",
      "Numero de steps en la trayectoria: 254 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.037404672056436536\n",
      "Trayectoria 76/200\n",
      "Numero de steps en la trayectoria: 278 -- Recompensa total del episodio: 3.0\n",
      "17/17 - loss: -0.07840013986124708\n",
      "Trayectoria 77/200\n",
      "Numero de steps en la trayectoria: 349 -- Recompensa total del episodio: 11.0\n",
      "21/21 - loss: 0.041258245174373889\n",
      "Trayectoria 78/200\n",
      "Numero de steps en la trayectoria: 281 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: -0.026964348247822595\n",
      "Trayectoria 79/200\n",
      "Numero de steps en la trayectoria: 276 -- Recompensa total del episodio: 4.0\n",
      "17/17 - loss: -0.008233455612378968\n",
      "Trayectoria 80/200\n",
      "Numero de steps en la trayectoria: 489 -- Recompensa total del episodio: 11.0\n",
      "30/30 - loss: 0.008693982039888689554\n",
      "Trayectoria 81/200\n",
      "Numero de steps en la trayectoria: 512 -- Recompensa total del episodio: 7.0\n",
      "32/32 - loss: 0.0160235961375292467\n",
      "Trayectoria 82/200\n",
      "Numero de steps en la trayectoria: 52 -- Recompensa total del episodio: 4.0\n",
      "3/3 - loss: 0.0461866036057472281\n",
      "Trayectoria 83/200\n",
      "Numero de steps en la trayectoria: 314 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: 0.0319839684586776414\n",
      "Trayectoria 84/200\n",
      "Numero de steps en la trayectoria: 284 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: 0.0561409669325632358\n",
      "Trayectoria 85/200\n",
      "Numero de steps en la trayectoria: 222 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.031421546179514672\n",
      "Trayectoria 86/200\n",
      "Numero de steps en la trayectoria: 282 -- Recompensa total del episodio: 4.0\n",
      "17/17 - loss: 0.046527835376122435\n",
      "Trayectoria 87/200\n",
      "Numero de steps en la trayectoria: 169 -- Recompensa total del episodio: 1.0\n",
      "10/10 - loss: -0.0566283542662859742\n",
      "Trayectoria 88/200\n",
      "Numero de steps en la trayectoria: 253 -- Recompensa total del episodio: 4.0\n",
      "15/15 - loss: 0.019549764941136055\n",
      "Trayectoria 89/200\n",
      "Numero de steps en la trayectoria: 155 -- Recompensa total del episodio: 1.0\n",
      "9/9 - loss: -0.026896218458811443\n",
      "Trayectoria 90/200\n",
      "Numero de steps en la trayectoria: 191 -- Recompensa total del episodio: 2.0\n",
      "11/11 - loss: 0.034753110259771375\n",
      "Trayectoria 91/200\n",
      "Numero de steps en la trayectoria: 189 -- Recompensa total del episodio: 2.0\n",
      "11/11 - loss: -0.022218532860279062\n",
      "Trayectoria 92/200\n",
      "Numero de steps en la trayectoria: 186 -- Recompensa total del episodio: 2.0\n",
      "11/11 - loss: -0.036302597007968215\n",
      "Trayectoria 93/200\n",
      "Numero de steps en la trayectoria: 262 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.04072163556702435\n",
      "Trayectoria 94/200\n",
      "Numero de steps en la trayectoria: 364 -- Recompensa total del episodio: 6.0\n",
      "22/22 - loss: 0.0053864830935543267\n",
      "Trayectoria 95/200\n",
      "Numero de steps en la trayectoria: 308 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: 0.009450962080767265\n",
      "Trayectoria 96/200\n",
      "Numero de steps en la trayectoria: 217 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.0485077096292605817\n",
      "Trayectoria 97/200\n",
      "Numero de steps en la trayectoria: 265 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.019710780587047347\n",
      "Trayectoria 98/200\n",
      "Numero de steps en la trayectoria: 255 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.015100238472223283\n",
      "Trayectoria 99/200\n",
      "Numero de steps en la trayectoria: 227 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: 0.012390639366848136\n",
      "Trayectoria 100/200\n",
      "Numero de steps en la trayectoria: 275 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: -0.005318373879965614\n",
      "Trayectoria 101/200\n",
      "Numero de steps en la trayectoria: 221 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.017500986560032916\n",
      "Trayectoria 102/200\n",
      "Numero de steps en la trayectoria: 305 -- Recompensa total del episodio: 7.0\n",
      "19/19 - loss: -0.036151115635507998\n",
      "Trayectoria 103/200\n",
      "Numero de steps en la trayectoria: 282 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.056001197765855234\n",
      "Trayectoria 104/200\n",
      "Numero de steps en la trayectoria: 253 -- Recompensa total del episodio: 3.0\n",
      "15/15 - loss: -0.03117577681938808\n",
      "Trayectoria 105/200\n",
      "Numero de steps en la trayectoria: 281 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: -0.043816088753588056\n",
      "Trayectoria 106/200\n",
      "Numero de steps en la trayectoria: 319 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: 0.0211476716948182935\n",
      "Trayectoria 107/200\n",
      "Numero de steps en la trayectoria: 311 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: 0.017710471231686444\n",
      "Trayectoria 108/200\n",
      "Numero de steps en la trayectoria: 256 -- Recompensa total del episodio: 3.0\n",
      "16/16 - loss: -0.05295830778777599\n",
      "Trayectoria 109/200\n",
      "Numero de steps en la trayectoria: 321 -- Recompensa total del episodio: 11.0\n",
      "20/20 - loss: 0.009534188732504841\n",
      "Trayectoria 110/200\n",
      "Numero de steps en la trayectoria: 314 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: -0.011214070610309906\n",
      "Trayectoria 111/200\n",
      "Numero de steps en la trayectoria: 268 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.0178458050359040543\n",
      "Trayectoria 112/200\n",
      "Numero de steps en la trayectoria: 274 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: 0.004882507683599704\n",
      "Trayectoria 113/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.03533140843113264\n",
      "Trayectoria 114/200\n",
      "Numero de steps en la trayectoria: 288 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: 0.0145156536665227672\n",
      "Trayectoria 115/200\n",
      "Numero de steps en la trayectoria: 269 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.031065165472682565\n",
      "Trayectoria 116/200\n",
      "Numero de steps en la trayectoria: 220 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.0035443414862339354\n",
      "Trayectoria 117/200\n",
      "Numero de steps en la trayectoria: 229 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: 0.0069253143987485386\n",
      "Trayectoria 118/200\n",
      "Numero de steps en la trayectoria: 276 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.0019650568856912493\n",
      "Trayectoria 119/200\n",
      "Numero de steps en la trayectoria: 284 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.006170134772272672\n",
      "Trayectoria 120/200\n",
      "Numero de steps en la trayectoria: 256 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.0007617846131324768\n",
      "Trayectoria 121/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.03577466557423275\n",
      "Trayectoria 122/200\n",
      "Numero de steps en la trayectoria: 267 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.019996341317892075\n",
      "Trayectoria 123/200\n",
      "Numero de steps en la trayectoria: 318 -- Recompensa total del episodio: 11.0\n",
      "19/19 - loss: 0.0204206377659973498\n",
      "Trayectoria 124/200\n",
      "Numero de steps en la trayectoria: 279 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: -0.026832098689149418\n",
      "Trayectoria 125/200\n",
      "Numero de steps en la trayectoria: 295 -- Recompensa total del episodio: 7.0\n",
      "18/18 - loss: 0.03537714646922219265\n",
      "Trayectoria 126/200\n",
      "Numero de steps en la trayectoria: 283 -- Recompensa total del episodio: 11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - loss: 0.022991910795955094\n",
      "Trayectoria 127/200\n",
      "Numero de steps en la trayectoria: 274 -- Recompensa total del episodio: 7.0\n",
      "17/17 - loss: -0.001164321513736962\n",
      "Trayectoria 128/200\n",
      "Numero de steps en la trayectoria: 298 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: 0.044536889427238046\n",
      "Trayectoria 129/200\n",
      "Numero de steps en la trayectoria: 257 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.0017901044338941574\n",
      "Trayectoria 130/200\n",
      "Numero de steps en la trayectoria: 249 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.016345014174779255\n",
      "Trayectoria 131/200\n",
      "Numero de steps en la trayectoria: 215 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.03338145550626975\n",
      "Trayectoria 132/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.014179625610510498\n",
      "Trayectoria 133/200\n",
      "Numero de steps en la trayectoria: 256 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.009930118918418884\n",
      "Trayectoria 134/200\n",
      "Numero de steps en la trayectoria: 215 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -5.7810774216284985e-05\n",
      "Trayectoria 135/200\n",
      "Numero de steps en la trayectoria: 248 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.0179060729841391286\n",
      "Trayectoria 136/200\n",
      "Numero de steps en la trayectoria: 274 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: 0.041838240097550786\n",
      "Trayectoria 137/200\n",
      "Numero de steps en la trayectoria: 276 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.005008729503435252\n",
      "Trayectoria 138/200\n",
      "Numero de steps en la trayectoria: 253 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.082943762590487867\n",
      "Trayectoria 139/200\n",
      "Numero de steps en la trayectoria: 252 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.010693615674972541\n",
      "Trayectoria 140/200\n",
      "Numero de steps en la trayectoria: 252 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.014556972558299753\n",
      "Trayectoria 141/200\n",
      "Numero de steps en la trayectoria: 263 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.03459403524175286\n",
      "Trayectoria 142/200\n",
      "Numero de steps en la trayectoria: 220 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.046976131888536304\n",
      "Trayectoria 143/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.005149236818154641\n",
      "Trayectoria 144/200\n",
      "Numero de steps en la trayectoria: 255 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.022546425958474486\n",
      "Trayectoria 145/200\n",
      "Numero de steps en la trayectoria: 250 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.057157092168927184\n",
      "Trayectoria 146/200\n",
      "Numero de steps en la trayectoria: 256 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.032019056845456367\n",
      "Trayectoria 147/200\n",
      "Numero de steps en la trayectoria: 260 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.0804717519204132353\n",
      "Trayectoria 148/200\n",
      "Numero de steps en la trayectoria: 250 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.0348767355084419186\n",
      "Trayectoria 149/200\n",
      "Numero de steps en la trayectoria: 278 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.020481303772505587\n",
      "Trayectoria 150/200\n",
      "Numero de steps en la trayectoria: 220 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.024656257377221044\n",
      "Trayectoria 151/200\n",
      "Numero de steps en la trayectoria: 194 -- Recompensa total del episodio: 2.0\n",
      "12/12 - loss: -0.01606644814213118\n",
      "Trayectoria 152/200\n",
      "Numero de steps en la trayectoria: 247 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.015621958673000334\n",
      "Trayectoria 153/200\n",
      "Numero de steps en la trayectoria: 213 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.05742816340464811\n",
      "Trayectoria 154/200\n",
      "Numero de steps en la trayectoria: 248 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.0166341227789719994\n",
      "Trayectoria 155/200\n",
      "Numero de steps en la trayectoria: 253 -- Recompensa total del episodio: 4.0\n",
      "15/15 - loss: 0.007663869857788098\n",
      "Trayectoria 156/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.016524784887830407\n",
      "Trayectoria 157/200\n",
      "Numero de steps en la trayectoria: 283 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.054969965096782244\n",
      "Trayectoria 158/200\n",
      "Numero de steps en la trayectoria: 262 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.08681805955711752\n",
      "Trayectoria 159/200\n",
      "Numero de steps en la trayectoria: 219 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.04875337676360057211\n",
      "Trayectoria 160/200\n",
      "Numero de steps en la trayectoria: 250 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.0125266137222449614\n",
      "Trayectoria 161/200\n",
      "Numero de steps en la trayectoria: 246 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.012807890400290484\n",
      "Trayectoria 162/200\n",
      "Numero de steps en la trayectoria: 221 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.09687136400204438\n",
      "Trayectoria 163/200\n",
      "Numero de steps en la trayectoria: 251 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.034915166844924295\n",
      "Trayectoria 164/200\n",
      "Numero de steps en la trayectoria: 248 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.04149870301286379\n",
      "Trayectoria 165/200\n",
      "Numero de steps en la trayectoria: 247 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.03895464092493057325\n",
      "Trayectoria 166/200\n",
      "Numero de steps en la trayectoria: 212 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.005750238035733873\n",
      "Trayectoria 167/200\n",
      "Numero de steps en la trayectoria: 249 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.0343431369711955494\n",
      "Trayectoria 168/200\n",
      "Numero de steps en la trayectoria: 215 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.0902401068462775342\n",
      "Trayectoria 169/200\n",
      "Numero de steps en la trayectoria: 219 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.053163299193749065\n",
      "Trayectoria 170/200\n",
      "Numero de steps en la trayectoria: 328 -- Recompensa total del episodio: 9.0\n",
      "20/20 - loss: 0.0259237476624548564\n",
      "Trayectoria 171/200\n",
      "Numero de steps en la trayectoria: 247 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.036302144949634865\n",
      "Trayectoria 172/200\n",
      "Numero de steps en la trayectoria: 256 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.018871057662181556\n",
      "Trayectoria 173/200\n",
      "Numero de steps en la trayectoria: 259 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.0211153127602301545\n",
      "Trayectoria 174/200\n",
      "Numero de steps en la trayectoria: 253 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.052049295604228996\n",
      "Trayectoria 175/200\n",
      "Numero de steps en la trayectoria: 253 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.022455144673585888\n",
      "Trayectoria 176/200\n",
      "Numero de steps en la trayectoria: 221 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.052973845257208996\n",
      "Trayectoria 177/200\n",
      "Numero de steps en la trayectoria: 258 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.04226232087239623\n",
      "Trayectoria 178/200\n",
      "Numero de steps en la trayectoria: 252 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.004755935569604228\n",
      "Trayectoria 179/200\n",
      "Numero de steps en la trayectoria: 250 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: -0.01747763492166996496\n",
      "Trayectoria 180/200\n",
      "Numero de steps en la trayectoria: 282 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: 0.0218515916563132262\n",
      "Trayectoria 181/200\n",
      "Numero de steps en la trayectoria: 217 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.11826611768740874\n",
      "Trayectoria 182/200\n",
      "Numero de steps en la trayectoria: 257 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: 0.015136721543967724\n",
      "Trayectoria 183/200\n",
      "Numero de steps en la trayectoria: 322 -- Recompensa total del episodio: 11.0\n",
      "20/20 - loss: 0.054316786397248504\n",
      "Trayectoria 184/200\n",
      "Numero de steps en la trayectoria: 223 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.0708774856936473408\n",
      "Trayectoria 185/200\n",
      "Numero de steps en la trayectoria: 300 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: -0.0175525885489251966\n",
      "Trayectoria 186/200\n",
      "Numero de steps en la trayectoria: 294 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: 0.0422665642367468865\n",
      "Trayectoria 187/200\n",
      "Numero de steps en la trayectoria: 226 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: -0.05515879658716065\n",
      "Trayectoria 188/200\n",
      "Numero de steps en la trayectoria: 282 -- Recompensa total del episodio: 11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - loss: 0.0339474235387409456\n",
      "Trayectoria 189/200\n",
      "Numero de steps en la trayectoria: 289 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: 0.0196195617318153382\n",
      "Trayectoria 190/200\n",
      "Numero de steps en la trayectoria: 216 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.012860997938192799\n",
      "Trayectoria 191/200\n",
      "Numero de steps en la trayectoria: 311 -- Recompensa total del episodio: 8.0\n",
      "19/19 - loss: 0.02652416025337420505\n",
      "Trayectoria 192/200\n",
      "Numero de steps en la trayectoria: 259 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.04008766042534262\n",
      "Trayectoria 193/200\n",
      "Numero de steps en la trayectoria: 275 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.021222049151273344\n",
      "Trayectoria 194/200\n",
      "Numero de steps en la trayectoria: 247 -- Recompensa total del episodio: 7.0\n",
      "15/15 - loss: 0.030032347018520045\n",
      "Trayectoria 195/200\n",
      "Numero de steps en la trayectoria: 257 -- Recompensa total del episodio: 7.0\n",
      "16/16 - loss: -0.025922424159944057\n",
      "Trayectoria 196/200\n",
      "Numero de steps en la trayectoria: 272 -- Recompensa total del episodio: 11.0\n",
      "17/17 - loss: -0.017779870068325723\n",
      "Trayectoria 197/200\n",
      "Numero de steps en la trayectoria: 219 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: 0.010033532403982594\n",
      "Trayectoria 198/200\n",
      "Numero de steps en la trayectoria: 220 -- Recompensa total del episodio: 3.0\n",
      "13/13 - loss: -0.073088813000000438\n",
      "Trayectoria 199/200\n",
      "Numero de steps en la trayectoria: 238 -- Recompensa total del episodio: 3.0\n",
      "14/14 - loss: 0.00011762657335826453\n",
      "Trayectoria 200/200\n",
      "Numero de steps en la trayectoria: 297 -- Recompensa total del episodio: 11.0\n",
      "18/18 - loss: -0.020853369910683907\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento de agente bPG:\n",
    "torch.manual_seed(123) # Reproducibilidad\n",
    "\n",
    "# Instanciamos un entorno\n",
    "env = gym.make(env_name)\n",
    "env.seed(123)\n",
    "number_actions = env.action_space.n\n",
    "\n",
    "# Instanciamos el modelo del actor\n",
    "model = Actor(number_actions=number_actions).to(device)\n",
    "model.eval() # en train, tendriamos que hacer: model.train()\n",
    "\n",
    "# Preparar optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "# Inicializamos primera trayectoria\n",
    "obs, state = env.reset(), None\n",
    "state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "\n",
    "# Primer bucle: episodios de entrenamiento\n",
    "for i_episode in range(int(EPISODES_TRAINING)):\n",
    "  done = False\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # A. Recopilar trayectoria\n",
    "\n",
    "  # Buffer de memoria (s,a,r,s)\n",
    "  s, a, r, lives = [], [], [], [] # Lives is enviroment-specific information\n",
    "\n",
    "  # Segundo bucle: recopilamos la trayectoria\n",
    "  for step in range(int(T)):\n",
    "\n",
    "    # Foward de actor dado el estado actual\n",
    "    with torch.no_grad():\n",
    "        logits = model(state.unsqueeze(0).to(device))\n",
    "\n",
    "    # Obtenemos probabilidad de acción\n",
    "    prob = torch.nn.functional.softmax(logits, -1) # [0, 0.2, 0.6, 0.2]\n",
    "\n",
    "    # Obtener la acción a realizar\n",
    "    action = prob.multinomial(num_samples=1) # 2\n",
    "\n",
    "    # Con la accion seleccionada, realizamos un step en el entorno\n",
    "    obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "    # Almacenamos información en memoria\n",
    "    a_ohe = torch.nn.functional.one_hot(action, num_classes=number_actions).squeeze(0) # Actions to ohe\n",
    "    s.append(state.unsqueeze(0).numpy()), a.append(a_ohe.numpy()), r.append(reward), lives.append(info['ale.lives'])\n",
    "    \n",
    "    # Actualizamos el estado con la siguiente observacion\n",
    "    state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "    \n",
    "    if done:\n",
    "      obs, state = env.reset(), None\n",
    "      state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "      break\n",
    "    \n",
    "  # Printear evolución\n",
    "  print(\"Trayectoria \" + str(int(i_episode)+1) + \"/\" + str(int(EPISODES_TRAINING)))\n",
    "  print(\"Numero de steps en la trayectoria: \" + str(len(r)) + \" -- Recompensa total del episodio: \" + str(np.sum(r)))\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # B. Preparar la información\n",
    "\n",
    "  # Computar los discount rewards\n",
    "  running_r = 0\n",
    "  discount_rewards = r.copy()\n",
    "  for i in reversed(range(len(r))):\n",
    "    if lives[i] != lives[i-1]:\n",
    "        running_r = 0\n",
    "    # Obtenemos discount rewards para cada t\n",
    "    running_r = r[i] + GAMMA * running_r\n",
    "    discount_rewards[i] = running_r\n",
    "    \n",
    "  # Normalización para reducir la varianza\n",
    "  discount_rewards = np.array(discount_rewards)\n",
    "  discount_rewards -= np.mean(discount_rewards)\n",
    "  discount_rewards /= (np.std(discount_rewards)+1e-3)\n",
    "    \n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # C. Entrenar policy con trayectoria recolectada\n",
    "  model.train()\n",
    "  BS = 16\n",
    "\n",
    "  idx, track_loss = 0, 0\n",
    "  indexes = np.arange(0, len(s))\n",
    "  np.random.shuffle(indexes)\n",
    "  for i_step in range(len(s)//BS):\n",
    "      indexes_batch = indexes[idx:idx+BS]\n",
    "\n",
    "      # Seleccionamos un batch de la trayectoria\n",
    "      states_batch = np_to_tensor(np.concatenate(s)[indexes_batch,:,:,:])\n",
    "      actions_batch = np_to_tensor(np.concatenate(a)[indexes_batch,:].squeeze())\n",
    "      relevance_batch = np_to_tensor(discount_rewards[indexes_batch])\n",
    "    \n",
    "      # Hacemos forward al actor-critic dado el estado actual\n",
    "      logits = model(states_batch)\n",
    "\n",
    "      # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "      log_softmax = torch.nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "      # Obtenemos criterios de optimización\n",
    "      policy_loss = - torch.mean((log_softmax * actions_batch.detach()).sum(-1) * relevance_batch.detach())\n",
    "    \n",
    "      # Computamos gradientes\n",
    "      policy_loss.backward()\n",
    "      # Actualizamos los pesos\n",
    "      optimizer.step()\n",
    "      # Limpiamos gradientes del modelo\n",
    "      optimizer.zero_grad()\n",
    "      # Actualizamos iterador de batch\n",
    "      idx += BS\n",
    "      track_loss += policy_loss.item()/(len(s)//BS)\n",
    "      \n",
    "      # Track losses\n",
    "      print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - loss: \" + str(policy_loss.item()), end=\"\\r\")\n",
    "        \n",
    "  # Overall loss\n",
    "  print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    " \n",
    "  # Guardar los pesos del modelo\n",
    "  torch.save(model.state_dict(), model_path)\n",
    "\n",
    "  # Set again model to eval\n",
    "  model.eval()\n",
    "\n",
    "# Guardar los pesos del modelo\n",
    "torch.save(model.state_dict(), \"last_\" + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/10, score: 10.0\n",
      "episode: 1/10, score: 11.0\n",
      "episode: 2/10, score: 4.0\n",
      "episode: 3/10, score: 7.0\n",
      "episode: 4/10, score: 7.0\n",
      "episode: 5/10, score: 11.0\n",
      "episode: 6/10, score: 4.0\n",
      "episode: 7/10, score: 11.0\n",
      "episode: 8/10, score: 7.0\n",
      "episode: 9/10, score: 7.0\n"
     ]
    }
   ],
   "source": [
    "# Inferencia en test\n",
    "env = gym.make(env_name)\n",
    "model.load_state_dict(torch.load(\"last_\" + model_path))\n",
    "test(env, model, EPISODES_TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3. Actor-Critic (AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTUREM83bSgA"
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "model_path = \"breackout_ac.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "XHCsfdsqcn_E"
   },
   "outputs": [],
   "source": [
    "# Definir arquitectura del knowledge (CNN)\n",
    "class ActorCritic(torch.nn.Module):\n",
    "  def __init__(self, number_actions=4):\n",
    "    super(ActorCritic, self).__init__()\n",
    "\n",
    "    # Definimos las capas de la red neuronal\n",
    "    # 1) Base Model\n",
    "    self.conv1 = torch.nn.Conv2d(4, 32, 8, stride=4)\n",
    "    self.conv2 = torch.nn.Conv2d(32, 64, 4, stride=2)\n",
    "    self.conv3 = torch.nn.Conv2d(64, 64, 3, stride=1)\n",
    "\n",
    "    # 2) Proyección intermedia (Asumimos que hemos hecho flatten)\n",
    "    self.fc1 = torch.nn.Linear(7*7*64, 512) # Para más generalizable, podria usarse GAP\n",
    "\n",
    "    # 3) Salida hibrida\n",
    "    self.actor = torch.nn.Linear(512, number_actions) # number of actions\n",
    "    self.critic = torch.nn.Linear(512, 1) # linear output of value\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # 1) Forward base model\n",
    "    x = torch.nn.functional.relu(self.conv1(x))\n",
    "    x = torch.nn.functional.relu(self.conv2(x))\n",
    "    x = torch.nn.functional.relu(self.conv3(x))\n",
    "\n",
    "    # 2) Flatten + Projection\n",
    "    x = x.view(-1, 7*7*64)\n",
    "    x = torch.nn.functional.relu(self.fc1(x))\n",
    "\n",
    "    # 3) Salida hibrida\n",
    "    policy = self.actor(x) # Aplicaremos softmax más adelante\n",
    "    value = self.critic(x)\n",
    "\n",
    "    return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de agente Actor-Critic:\n",
    "torch.manual_seed(123) # Reproducibilidad\n",
    "\n",
    "# Instanciamos un entorno\n",
    "env = gym.make(env_name)\n",
    "env.seed(123)\n",
    "number_actions = env.action_space.n\n",
    "\n",
    "# Instanciamos el modelo del actor\n",
    "model = ActorCritic(number_actions=number_actions).to(device)\n",
    "model.eval() # en train, tendriamos que hacer: model.train()\n",
    "\n",
    "# Preparar optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "# Inicializamos primera trayectoria\n",
    "obs, state = env.reset(), None\n",
    "state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "\n",
    "# Primer bucle: episodios de entrenamiento\n",
    "for i_episode in range(int(EPISODES_TRAINING)):\n",
    "  done = False\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # A. Recopilar trayectoria\n",
    "\n",
    "  # Buffer de memoria (s,a,r,s)\n",
    "  s, a, r, lives, values = [], [], [], [], [] # Lives is enviroment-specific information\n",
    "\n",
    "  # Segundo bucle: recopilamos la trayectoria\n",
    "  for step in range(int(T)):\n",
    "\n",
    "    # Foward de actor dado el estado actual\n",
    "    with torch.no_grad():\n",
    "        logits, v = model(state.unsqueeze(0).to(device))\n",
    "\n",
    "    # Obtenemos probabilidad de acción\n",
    "    prob = torch.nn.functional.softmax(logits, -1) # [0, 0.2, 0.6, 0.2]\n",
    "\n",
    "    # Obtener la acción a realizar\n",
    "    action = prob.multinomial(num_samples=1) # 2\n",
    "\n",
    "    # Con la accion seleccionada, realizamos un step en el entorno\n",
    "    obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "    # Almacenamos información en memoria\n",
    "    a_ohe = torch.nn.functional.one_hot(action, num_classes=number_actions).squeeze(0) # Actions to ohe\n",
    "    s.append(state.unsqueeze(0).numpy()), a.append(a_ohe.numpy()), r.append(reward), lives.append(info['ale.lives'])\n",
    "    values.append(v.item())\n",
    "    \n",
    "    # Actualizamos el estado con la siguiente observacion\n",
    "    state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "    \n",
    "    if done:\n",
    "      obs, state = env.reset(), None\n",
    "      state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "      break\n",
    "    \n",
    "  # Printear evolución\n",
    "  print(\"Trayectoria \" + str(int(i_episode)+1) + \"/\" + str(int(EPISODES_TRAINING)))\n",
    "  print(\"Numero de steps en la trayectoria: \" + str(len(r)) + \" -- Recompensa total del episodio: \" + str(np.sum(r)) + \n",
    "        \" -- Value promedio: \" + str(np.mean(values)))\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # B. Preparar la información\n",
    "    \n",
    "  # Recompensa en estado final (terminal o no terminal)\n",
    "  running_r = 0\n",
    "  if not done:\n",
    "    _, value = model(state.unsqueeze(0).to(device))\n",
    "    running_r = v.item()\n",
    "  \n",
    "  # Computar los discount rewards\n",
    "  discount_rewards = r.copy()\n",
    "  for i in reversed(range(len(r))):\n",
    "    if lives[i] != lives[i-1]:\n",
    "        running_r = 0\n",
    "    # Obtenemos discount rewards para cada t\n",
    "    running_r = r[i] + GAMMA * running_r\n",
    "    discount_rewards[i] = running_r\n",
    "    \n",
    "  # Normalización para reducir la varianza\n",
    "  discount_rewards = np.array(discount_rewards)\n",
    "  discount_rewards -= np.mean(discount_rewards)\n",
    "  discount_rewards /= (np.std(discount_rewards)+1e-3)\n",
    "    \n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # C. Entrenar actor y critic\n",
    "    \n",
    "  # C.1. Entrenamos actor utilizando la trayectoria recolectada\n",
    "  model.train()\n",
    "  BS = 16\n",
    "\n",
    "  idx, track_loss = 0, 0\n",
    "  indexes = np.arange(0, len(s))\n",
    "  np.random.shuffle(indexes)\n",
    "  for i_step in range(len(s)//BS):\n",
    "      indexes_batch = indexes[idx:idx+BS]\n",
    "\n",
    "      # Seleccionamos un batch de la trayectoria\n",
    "      states_batch = np_to_tensor(np.concatenate(s)[indexes_batch,:,:,:])\n",
    "      actions_batch = np_to_tensor(np.concatenate(a)[indexes_batch,:].squeeze())\n",
    "      relevance_batch = np_to_tensor(np.array(values)[indexes_batch])\n",
    "    \n",
    "      # Hacemos forward al actor-critic dado el estado actual\n",
    "      logits, _ = model(states_batch)\n",
    "\n",
    "      # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "      log_softmax = torch.nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "      # Obtenemos criterios de optimización\n",
    "      policy_loss = - torch.mean((log_softmax * actions_batch.detach()).sum(-1) * relevance_batch.detach())\n",
    "\n",
    "      # Computamos gradientes\n",
    "      policy_loss.backward()\n",
    "      # Actualizamos los pesos\n",
    "      optimizer.step()\n",
    "      # Limpiamos gradientes del modelo\n",
    "      optimizer.zero_grad()\n",
    "      # Actualizamos iterador de batch\n",
    "      idx += BS\n",
    "      track_loss += policy_loss.item()/(len(s)//BS)\n",
    "      \n",
    "      # Track losses\n",
    "      print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - policy loss: \" + str(policy_loss.item()), end=\"\\r\")\n",
    "\n",
    "  # C.2. Entrenamos el critic con las discount rewards\n",
    "  model.train()\n",
    "  BS = 16\n",
    "\n",
    "  idx, track_loss_value = 0, 0\n",
    "  indexes = np.arange(0, len(s))\n",
    "  np.random.shuffle(indexes)\n",
    "  for i_step in range(len(s)//BS):\n",
    "      indexes_batch = indexes[idx:idx+BS]\n",
    "\n",
    "      # Seleccionamos un batch de la trayectoria\n",
    "      states_batch = np_to_tensor(np.concatenate(s)[indexes_batch,:,:,:])\n",
    "      target_value = np_to_tensor(discount_rewards[indexes_batch])\n",
    "    \n",
    "      # Hacemos forward al actor-critic dado el estado actual\n",
    "      _, values = model(states_batch)\n",
    "\n",
    "      # Obtenemos criterios de optimización\n",
    "      value_loss = torch.mean((target_value - values).pow(2))\n",
    "    \n",
    "      # Computamos gradientes\n",
    "      value_loss.backward()\n",
    "      # Actualizamos los pesos\n",
    "      optimizer.step()\n",
    "      # Limpiamos gradientes del modelo\n",
    "      optimizer.zero_grad()\n",
    "      # Actualizamos iterador de batch\n",
    "      idx += BS\n",
    "      track_loss_value += value_loss.item()/(len(s)//BS)\n",
    "      \n",
    "      # Track losses\n",
    "      print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - value loss: \" + str(value_loss.item()), end=\"\\r\")\n",
    "    \n",
    "  # Overall loss\n",
    "  print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - policy loss: \" + str(track_loss) +\n",
    "        \" - value loss: \" + str(track_loss_value), end=\"\\n\")\n",
    " \n",
    "  # Guardar los pesos del modelo\n",
    "  torch.save(model.state_dict(), model_path)\n",
    "\n",
    "  # Set again model to eval\n",
    "  model.eval()\n",
    "\n",
    "# Guardar los pesos del modelo\n",
    "torch.save(model.state_dict(), \"last_\" + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testeo\n",
    "def test(env, model, runs):\n",
    "    model.eval()\n",
    "\n",
    "    for e in range(runs):\n",
    "        obs, state = env.reset(), None\n",
    "        state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "        done, score = False, 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            \n",
    "            # Al testear, seleccionamos la acción con mayor probabilidad (no muestreo)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = model(state.unsqueeze(0).to(device))\n",
    "                # Obtenemos probabilidad de acción\n",
    "                prob = torch.nn.functional.softmax(logits, -1)\n",
    "                # Obtener la acción a realizar\n",
    "                action = prob.multinomial(num_samples=1)\n",
    "                \n",
    "            # Con la accion seleccionada, realizamos un step en el entorno\n",
    "            obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "            # Actualizamos el estado con la siguiente observacion\n",
    "            state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                env.close()\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, runs, score))\n",
    "                break\n",
    "    \n",
    "# Inferencia en test\n",
    "env = gym.make(env_name)\n",
    "model.load_state_dict(torch.load(\"last_\" + model_path))\n",
    "test(env, model, EPISODES_TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3. Advantadge Actor-Critic (A2C)\n",
    "(sin multiproceso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "model_path = \"breackout_a2c_sp.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de agente A2C (single process):\n",
    "torch.manual_seed(123) # Reproducibilidad\n",
    "\n",
    "# Instanciamos un entorno\n",
    "#env = gym.make(env_name)\n",
    "env = gym.make(env_name, repeat_action_probability=0.0, frameskip=(1,2))\n",
    "env.seed(123)\n",
    "number_actions = env.action_space.n\n",
    "\n",
    "# Instanciamos el modelo del actor\n",
    "model = ActorCritic(number_actions=number_actions).to(device)\n",
    "model.eval() # en train, tendriamos que hacer: model.train()\n",
    "\n",
    "# Preparar optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "# Inicializamos primera trayectoria\n",
    "obs, state = env.reset(), None\n",
    "state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "\n",
    "# Primer bucle: episodios de entrenamiento\n",
    "for i_episode in range(int(EPISODES_TRAINING)):\n",
    "  done = False\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # A. Recopilar trayectoria\n",
    "\n",
    "  # Buffer de memoria (s,a,r,s)\n",
    "  s, a, r, lives, values = [], [], [], [], [] # Lives is enviroment-specific information\n",
    "\n",
    "  # Segundo bucle: recopilamos la trayectoria\n",
    "  for step in range(int(T)):\n",
    "\n",
    "    # Foward de actor dado el estado actual\n",
    "    with torch.no_grad():\n",
    "        logits, v = model(state.unsqueeze(0).to(device))\n",
    "\n",
    "    # Obtenemos probabilidad de acción\n",
    "    prob = torch.nn.functional.softmax(logits, -1) # [0, 0.2, 0.6, 0.2]\n",
    "\n",
    "    # Obtener la acción a realizar\n",
    "    action = prob.multinomial(num_samples=1) # 2\n",
    "\n",
    "    # Con la accion seleccionada, realizamos un step en el entorno\n",
    "    obs, reward, done, info = env.step(action.item())\n",
    "\n",
    "    # Almacenamos información en memoria\n",
    "    a_ohe = torch.nn.functional.one_hot(action, num_classes=number_actions).squeeze(0) # Actions to ohe\n",
    "    s.append(state.unsqueeze(0).numpy()), a.append(a_ohe.numpy()), r.append(reward), lives.append(info['ale.lives'])\n",
    "    values.append(v.item())\n",
    "    \n",
    "    # Actualizamos el estado con la siguiente observacion\n",
    "    state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "    \n",
    "    if done:\n",
    "      obs, state = env.reset(), None\n",
    "      state = update_frame_sequence(state=state, obs=obs, width=WIDTH, height=HEIGHT)\n",
    "      break\n",
    "    \n",
    "  # Printear evolución\n",
    "  print(\"Trayectoria \" + str(int(i_episode)+1) + \"/\" + str(int(EPISODES_TRAINING)))\n",
    "  print(\"Numero de steps en la trayectoria: \" + str(len(r)) + \" -- Recompensa total del episodio: \" + str(np.sum(r)) + \n",
    "        \" -- Value promedio: \" + str(np.mean(values)))\n",
    "\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # B. Preparar la información\n",
    "    \n",
    "  # Recompensa en estado final (terminal o no terminal)\n",
    "  running_r = 0\n",
    "  if not done:\n",
    "    _, value = model(state.unsqueeze(0).to(device))\n",
    "    running_r = v.item()\n",
    "  \n",
    "  # Computar los discount rewards\n",
    "  discount_rewards = r.copy()\n",
    "  for i in reversed(range(len(r))):\n",
    "    if lives[i] != lives[i-1]:\n",
    "        running_r = 0\n",
    "    # Obtenemos discount rewards para cada t\n",
    "    running_r = r[i] + GAMMA * running_r\n",
    "    discount_rewards[i] = running_r\n",
    "    \n",
    "  # Normalización para reducir la varianza\n",
    "  discount_rewards = np.array(discount_rewards)\n",
    "  discount_rewards -= np.mean(discount_rewards)\n",
    "  discount_rewards /= (np.std(discount_rewards)+1e-3)\n",
    "  \n",
    "  # Calculate advantadge\n",
    "  advantadge = discount_rewards - np.array(values)\n",
    "    \n",
    "  # ------------------------------------------------------------------------------------\n",
    "  # C. Entrenar actor y critic\n",
    "    \n",
    "  # C.1. Entrenamos actor utilizando la trayectoria recolectada\n",
    "  model.train()\n",
    "  BS = 16\n",
    "\n",
    "  idx, track_loss = 0, 0\n",
    "  indexes = np.arange(0, len(s))\n",
    "  np.random.shuffle(indexes)\n",
    "  for i_step in range(len(s)//BS):\n",
    "      indexes_batch = indexes[idx:idx+BS]\n",
    "\n",
    "      # Seleccionamos un batch de la trayectoria\n",
    "      states_batch = np_to_tensor(np.concatenate(s)[indexes_batch,:,:,:])\n",
    "      actions_batch = np_to_tensor(np.concatenate(a)[indexes_batch,:].squeeze())\n",
    "      relevance_batch = np_to_tensor(advantadge[indexes_batch])\n",
    "    \n",
    "      # Hacemos forward al actor-critic dado el estado actual\n",
    "      logits, _ = model(states_batch)\n",
    "\n",
    "      # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "      log_softmax = torch.nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "      # Obtenemos criterios de optimización\n",
    "      policy_loss = - torch.mean((log_softmax * actions_batch.detach()).sum(-1) * relevance_batch.detach())\n",
    "        \n",
    "      # Computamos gradientes\n",
    "      policy_loss.backward()\n",
    "      # Actualizamos los pesos\n",
    "      #optimizer.step()\n",
    "      # Limpiamos gradientes del modelo\n",
    "      #optimizer.zero_grad()\n",
    "      # Actualizamos iterador de batch\n",
    "      idx += BS\n",
    "      track_loss += policy_loss.item()/(len(s)//BS)\n",
    "      \n",
    "      # Track losses\n",
    "      print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - policy loss: \" + str(policy_loss.item()), end=\"\\r\")\n",
    "\n",
    "  # C.2. Entrenamos el critic con las discount rewards\n",
    "  model.train()\n",
    "  BS = 16\n",
    "\n",
    "  idx, track_loss_value = 0, 0\n",
    "  indexes = np.arange(0, len(s))\n",
    "  np.random.shuffle(indexes)\n",
    "  for i_step in range(len(s)//BS):\n",
    "      indexes_batch = indexes[idx:idx+BS]\n",
    "\n",
    "      # Seleccionamos un batch de la trayectoria\n",
    "      states_batch = np_to_tensor(np.concatenate(s)[indexes_batch,:,:,:])\n",
    "      target_value = np_to_tensor(discount_rewards[indexes_batch])\n",
    "    \n",
    "      # Hacemos forward al actor-critic dado el estado actual\n",
    "      _, values = model(states_batch)\n",
    "\n",
    "      # Obtenemos criterios de optimización\n",
    "      value_loss = torch.mean((target_value - values).pow(2))\n",
    "    \n",
    "      # Computamos gradientes\n",
    "      value_loss.backward()\n",
    "      # Actualizamos los pesos\n",
    "      #optimizer.step()\n",
    "      # Limpiamos gradientes del modelo\n",
    "      #optimizer.zero_grad()\n",
    "      # Actualizamos iterador de batch\n",
    "      idx += BS\n",
    "      track_loss_value += value_loss.item()/(len(s)//BS)\n",
    "      \n",
    "      # Track losses\n",
    "      print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - value loss: \" + str(value_loss.item()), end=\"\\r\")\n",
    "    \n",
    "  # Actualizamos los pesos\n",
    "  optimizer.step()\n",
    "  # Limpiamos gradientes del modelo\n",
    "  optimizer.zero_grad() \n",
    "    \n",
    "  # Overall loss\n",
    "  print(str(i_step+1) + \"/\" + str(len(s)//BS) + \" - policy loss: \" + str(track_loss) +\n",
    "        \" - value loss: \" + str(track_loss_value), end=\"\\n\")\n",
    " \n",
    "  # Guardar los pesos del modelo\n",
    "  torch.save(model.state_dict(), model_path)\n",
    "\n",
    "  # Set again model to eval\n",
    "  model.eval()\n",
    "\n",
    "# Guardar los pesos del modelo\n",
    "torch.save(model.state_dict(), \"last_\" + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencia en test\n",
    "env = gym.make(env_name, repeat_action_probability=0, frameskip=1)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "test(env, model, EPISODES_TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
